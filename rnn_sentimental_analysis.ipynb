{"cells":[{"cell_type":"markdown","metadata":{"id":"2df898a7"},"source":["### Import Libaries"],"id":"2df898a7"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24111,"status":"ok","timestamp":1651340270544,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"s9IpGz9gGEsy","outputId":"34e53ac4-4759-4bed-d3a2-029252ef7835"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"s9IpGz9gGEsy"},{"cell_type":"markdown","metadata":{"id":"XeEDHhdmGPa1"},"source":["### Import Libraries"],"id":"XeEDHhdmGPa1"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1651340270545,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"09afbc95"},"outputs":[],"source":["import numpy as np\n","from collections import Counter\n","\n"],"id":"09afbc95"},{"cell_type":"markdown","metadata":{"id":"0b5adaa8"},"source":["### Load Data"],"id":"0b5adaa8"},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1616,"status":"ok","timestamp":1651340272153,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"5d9d3340"},"outputs":[],"source":["# read data from text files\n","with open('drive/MyDrive/Colab Notebooks/reviews.txt', 'r') as f:\n","    reviews = f.read()\n","with open('drive/MyDrive/Colab Notebooks/labels.txt', 'r') as f:\n","    labels = f.read()"],"id":"5d9d3340"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651340272154,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"c3039343","outputId":"5c7e7bc1-6277-44c7-eac6-4851d5b19f76"},"outputs":[{"output_type":"stream","name":"stdout","text":["bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n","story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n","\n","positive\n","negative\n","positive\n"]}],"source":["print(reviews[:1000])\n","print()\n","print(labels[:26])"],"id":"c3039343"},{"cell_type":"markdown","metadata":{"id":"c54867ff"},"source":["### Data Preprocessing"],"id":"c54867ff"},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3738,"status":"ok","timestamp":1651340275882,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"fe26a57e"},"outputs":[],"source":["from string import punctuation\n","\n","# get rid of punctuation\n","reviews = reviews.lower() # lowercase, standardize\n","all_text = ''.join([c for c in reviews if c not in punctuation])\n","\n","# split by new lines and spaces\n","reviews_split = all_text.split('\\n')\n","all_text = ' '.join(reviews_split)\n","\n","# create a list of words\n","words = all_text.split()"],"id":"fe26a57e"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1651340276214,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"4e73355b","outputId":"21867bd6-ccde-4d6b-d898-5de51841c9bc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bromwell',\n"," 'high',\n"," 'is',\n"," 'a',\n"," 'cartoon',\n"," 'comedy',\n"," 'it',\n"," 'ran',\n"," 'at',\n"," 'the',\n"," 'same',\n"," 'time',\n"," 'as',\n"," 'some',\n"," 'other',\n"," 'programs',\n"," 'about',\n"," 'school',\n"," 'life',\n"," 'such',\n"," 'as',\n"," 'teachers',\n"," 'my',\n"," 'years',\n"," 'in',\n"," 'the',\n"," 'teaching',\n"," 'profession',\n"," 'lead',\n"," 'me']"]},"metadata":{},"execution_count":6}],"source":["words[:30]"],"id":"4e73355b"},{"cell_type":"markdown","metadata":{"id":"19ce3ca6"},"source":["#### Encode word"],"id":"19ce3ca6"},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2503,"status":"ok","timestamp":1651340278713,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"663e366e"},"outputs":[],"source":["## Build a dictionary that maps words to integers\n","counts = Counter(words)\n","vocab = sorted(counts, key=counts.get, reverse=True)\n","vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n","\n","## use the dict to tokenize each review in reviews_split\n","## store the tokenized reviews in reviews_ints\n","reviews_ints = []\n","for review in reviews_split:\n","    reviews_ints.append([vocab_to_int[word] for word in review.split()])"],"id":"663e366e"},{"cell_type":"markdown","metadata":{"id":"fd4a2ce8"},"source":["##### Test Code"],"id":"fd4a2ce8"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1651340278715,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"edd68cc2","outputId":"71330f8d-54a9-4624-c8f0-0e7f4271aafe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words:  74072\n","\n","Tokenized review: \n"," [[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23]]\n"]}],"source":["# stats about vocabulary\n","print('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\n","print()\n","\n","# print tokens in first review\n","print('Tokenized review: \\n', reviews_ints[:1])"],"id":"edd68cc2"},{"cell_type":"markdown","metadata":{"id":"93593363"},"source":["#### Encode Labels"],"id":"93593363"},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1651340278716,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"bd329861"},"outputs":[],"source":["# 1=positive, 0=negative label conversion\n","labels_split = labels.split('\\n')\n","encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"],"id":"bd329861"},{"cell_type":"markdown","metadata":{"id":"20dc0898"},"source":["#### Remove Outliers"],"id":"20dc0898"},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1651340278717,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"04dd4dcb","outputId":"0f84f951-f91e-4a46-a125-8aef423a3b48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Zero-length reviews: 0\n","Maximum review length: 2514\n"]}],"source":["# outlier review stats\n","review_lens = Counter([len(x) for x in reviews_ints])\n","print(\"Zero-length reviews: {}\".format(review_lens[0]))\n","print(\"Maximum review length: {}\".format(max(review_lens)))"],"id":"04dd4dcb"},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651340278719,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"50bdc434","outputId":"2c57fb39-c9e3-49cf-e663-6056f180ed3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reviews before removing outliers:  25000\n","Number of reviews after removing outliers:  25000\n"]}],"source":["print('Number of reviews before removing outliers: ', len(reviews_ints))\n","\n","## remove any reviews/labels with zero length from the reviews_ints list.\n","\n","# get indices of any reviews with length 0\n","non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n","\n","# remove 0-length reviews and their labels\n","reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n","encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n","\n","print('Number of reviews after removing outliers: ', len(reviews_ints))"],"id":"50bdc434"},{"cell_type":"markdown","metadata":{"id":"789d8ade"},"source":["### Padding Sequences"],"id":"789d8ade"},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651340278721,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"d0dbee94"},"outputs":[],"source":["def pad_features(reviews_ints, seq_length):\n","    ''' Return features of review_ints, where each review is padded with 0's \n","        or truncated to the input seq_length.\n","    '''\n","    \n","    # getting the correct rows x cols shape\n","    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n","\n","    # for each review, I grab that review and \n","    for i, row in enumerate(reviews_ints):\n","        features[i, -len(row):] = np.array(row)[:seq_length]\n","    \n","    return features"],"id":"d0dbee94"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":538,"status":"ok","timestamp":1651340279243,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"2b2c4963","outputId":"d9392b32-d07f-42b7-8a8a-d0999c1fb780"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [22382    42 46418    15   706 17139  3389    47    77    35]\n"," [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   54    10    14   116    60   798   552    71   364     5]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    1   330   578    34     3   162   748  2731     9   325]\n"," [    9    11 10171  5305  1946   689   444    22   280   673]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   21   122  2069  1565   515  8181    88     6  1325  1182]\n"," [    1    20     6    76    40     6    58    81    95     5]\n"," [   54    10    84   329 26230 46427    63    10    14   614]\n"," [   11    20     6    30  1436 32317  3769   690 15100     6]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   40    26   109 17952  1422     9     1   327     4   125]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [   10   499     1   307 10399    55    74     8    13    30]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]\n"," [    0     0     0     0     0     0     0     0     0     0]]\n"]}],"source":["# Test your implementation!\n","\n","seq_length = 200\n","\n","features = pad_features(reviews_ints, seq_length=seq_length)\n","\n","## test statements - do not change - ##\n","assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n","assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n","\n","# print first 10 values of the first 30 batches \n","print(features[:30,:10])"],"id":"2b2c4963"},{"cell_type":"markdown","metadata":{"id":"3c84bbc0"},"source":["### Training, Validation, Test"],"id":"3c84bbc0"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651340279245,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"e88183a6","outputId":"55facb4d-8365-40b1-cfc0-84b08676d590"},"outputs":[{"output_type":"stream","name":"stdout","text":["\t\t\tFeature Shapes:\n","Train set: \t\t(20000, 200) \n","Validation set: \t(2500, 200) \n","Test set: \t\t(2500, 200)\n"]}],"source":["split_frac = 0.8\n","\n","## split data into training, validation, and test data (features and labels, x and y)\n","\n","split_idx = int(len(features)*split_frac)\n","train_x, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","\n","## print out the shapes of your resultant feature data\n","print(\"\\t\\t\\tFeature Shapes:\")\n","print(\"Train set: \\t\\t{}\".format(train_x.shape), \n","      \"\\nValidation set: \\t{}\".format(val_x.shape),\n","      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"],"id":"e88183a6"},{"cell_type":"markdown","metadata":{"id":"678a1104"},"source":["### Dataloaders and Batching"],"id":"678a1104"},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2475,"status":"ok","timestamp":1651340281712,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"4fe02f21"},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","# dataloaders\n","batch_size = 50\n","\n","# make sure the SHUFFLE your training data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"],"id":"4fe02f21"},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651340281713,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"7ca92c5a","outputId":"55f353d8-ae7a-4310-f8bb-6f7b0b7f6bbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample input size:  torch.Size([50, 200])\n","Sample input: \n"," tensor([[    0,     0,     0,  ...,     3,   248,    18],\n","        [    0,     0,     0,  ...,     5,    29,    68],\n","        [   10,    61,  2274,  ...,   628,  5131, 28696],\n","        ...,\n","        [17731,  7253,    13,  ...,    77,     3,  1406],\n","        [    0,     0,     0,  ...,     3,  2184, 15917],\n","        [   46,    21,    70,  ...,    97,    29,  1168]])\n","\n","Sample label size:  torch.Size([50])\n","Sample label: \n"," tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n","        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n","        1, 1])\n"]}],"source":["# obtain one batch of training data\n","dataiter = iter(train_loader)\n","sample_x, sample_y = dataiter.next()\n","\n","print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n","print('Sample input: \\n', sample_x)\n","print()\n","print('Sample label size: ', sample_y.size()) # batch_size\n","print('Sample label: \\n', sample_y)"],"id":"7ca92c5a"},{"cell_type":"markdown","metadata":{"id":"a50ab18a"},"source":["### Model Architecture"],"id":"a50ab18a"},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651340282092,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"9131f4de"},"outputs":[],"source":["import torch.nn as nn\n","\n","class SentimentRNN(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super(SentimentRNN, self).__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear and sigmoid layers\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        self.sig = nn.Sigmoid()\n","        \n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # embeddings and lstm_out\n","        x = x.long()\n","        embeds = self.embedding(x)\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        \n","        lstm_out = lstm_out[:, -1, :] # getting the last time step output\n","        \n","        # dropout and fully-connected layer\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        # sigmoid function\n","        sig_out = self.sig(out)\n","        \n","        # return last sigmoid output and hidden state\n","        return sig_out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","    \n","        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden"],"id":"9131f4de"},{"cell_type":"markdown","metadata":{"id":"0790a3f2"},"source":["### Instantiate Network"],"id":"0790a3f2"},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362,"status":"ok","timestamp":1651340282442,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"a4e915a3","outputId":"7ec7f422-d570-4e13-8c92-b24d23b8189c"},"outputs":[{"output_type":"stream","name":"stdout","text":["SentimentRNN(\n","  (embedding): Embedding(74073, 400)\n","  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=256, out_features=1, bias=True)\n","  (sig): Sigmoid()\n",")\n"]}],"source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n","output_size = 1\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 2\n","\n","net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","\n","print(net)"],"id":"a4e915a3"},{"cell_type":"markdown","metadata":{"id":"ab7413f1"},"source":["### Training"],"id":"ab7413f1"},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1651340282443,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"},"user_tz":-60},"id":"ad640549"},"outputs":[],"source":["# loss and optimization functions\n","lr=0.001\n","\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)"],"id":"ad640549"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ef6d3f6","outputId":"51f97be7-e552-4cc8-f1c0-36c30317c5db","executionInfo":{"status":"ok","timestamp":1651343667333,"user_tz":-60,"elapsed":3384895,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/3... Step: 100... Loss: 0.708170... Val Loss: 0.644943\n","Epoch: 1/3... Step: 200... Loss: 0.502061... Val Loss: 0.592746\n","Epoch: 1/3... Step: 300... Loss: 0.662993... Val Loss: 0.671184\n","Epoch: 1/3... Step: 400... Loss: 0.565309... Val Loss: 0.717004\n","Epoch: 2/3... Step: 500... Loss: 0.469627... Val Loss: 0.599777\n","Epoch: 2/3... Step: 600... Loss: 0.532908... Val Loss: 0.551364\n","Epoch: 2/3... Step: 700... Loss: 0.537345... Val Loss: 0.513203\n","Epoch: 2/3... Step: 800... Loss: 0.378576... Val Loss: 0.473986\n","Epoch: 3/3... Step: 900... Loss: 0.388025... Val Loss: 0.508637\n","Epoch: 3/3... Step: 1000... Loss: 0.259758... Val Loss: 0.480520\n","Epoch: 3/3... Step: 1100... Loss: 0.232538... Val Loss: 0.449716\n","Epoch: 3/3... Step: 1200... Loss: 0.467394... Val Loss: 0.440771\n"]}],"source":["# training params\n","\n","epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping\n","\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","\n","        # calculate the loss and perform backprop\n","        loss = criterion(output.squeeze(), labels.float())\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","               \n","                inputs, labels = inputs, labels\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output.squeeze(), labels.float())\n","\n","                val_losses.append(val_loss.item())\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n","            \n","            \n","            '''# save model if validation loss decreases\n","            valid_loss = np.mean(val_losses)\n","            v =[1]\n","            v.append(valid_loss)\n","            valid_loss_min = v[-2]\n","\n","            if valid_loss <= valid_loss_min:\n","              print(f'Validation loss decreased({valid_loss_min:.6f} -->{valid_loss:.6f}). Saving model...')\n","              model_name = 'rnn_4.0_epoch.pth'\n","              checkpoint = {'output_size': net.output_size,\n","                            'vocab_size':vocab_size,\n","                            'embedding_dim': embedding_dim,\n","                            'hidden_dim': net.hidden_dim,\n","                            'n_layers': net.n_layers,\n","                            'state_dict': net.state_dict()}\n","\n","              with open(model_name, 'wb') as f:\n","                torch.save(checkpoint, f)'''\n"],"id":"8ef6d3f6"},{"cell_type":"markdown","metadata":{"id":"c23bf00f"},"source":["### Testing"],"id":"c23bf00f"},{"cell_type":"code","execution_count":21,"metadata":{"id":"f7b464b0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695559,"user_tz":-60,"elapsed":28234,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"5e0ea8f6-7f54-493b-f36f-831da854d19a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.448\n","Test accuracy: 0.813\n"]}],"source":["# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    \n","    # get predicted outputs\n","    output, h = net(inputs, h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output.squeeze(), labels.float())\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.float().view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy())\n","    num_correct += np.sum(correct)\n","\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))"],"id":"f7b464b0"},{"cell_type":"code","execution_count":22,"metadata":{"id":"efdd6310","executionInfo":{"status":"ok","timestamp":1651343695560,"user_tz":-60,"elapsed":28,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}}},"outputs":[],"source":["# negative test review\n","test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'"],"id":"efdd6310"},{"cell_type":"code","execution_count":23,"metadata":{"id":"14c0308b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695561,"user_tz":-60,"elapsed":28,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"125435da-e2d4-43d8-8a65-ae7e09d930bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 247, 18, 10, 28, 108, 113, 14, 388, 2, 10, 181, 60, 273, 144, 11, 18, 68, 76, 113, 2, 1, 410, 14, 539]]\n"]}],"source":["from string import punctuation\n","\n","def tokenize_review(test_review):\n","    test_review = test_review.lower() # lowercase\n","    # get rid of punctuation\n","    test_text = ''.join([c for c in test_review if c not in punctuation])\n","\n","    # splitting by spaces\n","    test_words = test_text.split()\n","\n","    # tokens\n","    test_ints = []\n","    test_ints.append([ vocab_to_int.get(word, 0) for word in test_words])\n","\n","    return test_ints\n","\n","# test code and generate tokenized review\n","test_ints = tokenize_review(test_review_neg)\n","print(test_ints)"],"id":"14c0308b"},{"cell_type":"code","execution_count":24,"metadata":{"id":"c2503f05","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695562,"user_tz":-60,"elapsed":27,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"23f30851-581d-436e-ab49-3faea0f07850"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   1 247  18  10  28\n","  108 113  14 388   2  10 181  60 273 144  11  18  68  76 113   2   1 410\n","   14 539]]\n"]}],"source":["# test sequence padding\n","seq_length=200\n","features = pad_features(test_ints, seq_length)\n","\n","print(features)"],"id":"c2503f05"},{"cell_type":"code","execution_count":25,"metadata":{"id":"f658bc3c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695564,"user_tz":-60,"elapsed":25,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"b624e20b-1006-4b99-9d74-1a292fd3e968"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 200])\n"]}],"source":["# test conversion to tensor and pass into your model\n","feature_tensor = torch.from_numpy(features)\n","print(feature_tensor.size())"],"id":"f658bc3c"},{"cell_type":"code","execution_count":26,"metadata":{"id":"8baa289d","executionInfo":{"status":"ok","timestamp":1651343695564,"user_tz":-60,"elapsed":23,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}}},"outputs":[],"source":["def predict(net, test_review, sequence_length=200):\n","    \n","    net.eval()\n","    \n","    # tokenize review\n","    test_ints = tokenize_review(test_review)\n","    \n","    # pad tokenized sequence\n","    seq_length=sequence_length\n","    features = pad_features(test_ints, seq_length)\n","    \n","    # convert to tensor to pass into your model\n","    feature_tensor = torch.from_numpy(features)\n","    \n","    batch_size = feature_tensor.size(0)\n","    \n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","    \n","    \n","    # get the output from the model\n","    output, h = net(feature_tensor, h)\n","    \n","    # convert output probabilities to predicted class (0 or 1)\n","    pred = torch.round(output.squeeze()) \n","    # printing output value, before rounding\n","    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n","    \n","    # print custom response\n","    if(pred.item()==1):\n","        print(\"Positive review detected!\")\n","    else:\n","        print(\"Negative review detected.\")"],"id":"8baa289d"},{"cell_type":"code","execution_count":27,"metadata":{"id":"451d8c4d","executionInfo":{"status":"ok","timestamp":1651343695565,"user_tz":-60,"elapsed":24,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}}},"outputs":[],"source":["# positive test review\n","test_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'"],"id":"451d8c4d"},{"cell_type":"code","execution_count":28,"metadata":{"id":"8f7b3b84","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695565,"user_tz":-60,"elapsed":23,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"b046d88a-3c07-43e6-96f4-021220396bd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction value, pre-rounding: 0.016801\n","Negative review detected.\n"]}],"source":["# call function\n","seq_length=200 # good to use the length that was trained on\n","\n","predict(net, test_review_neg, seq_length)"],"id":"8f7b3b84"},{"cell_type":"code","source":["seq_length = 200\n","predict(net, test_review_pos, seq_length)"],"metadata":{"id":"rd_8lrXpPab9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651343695566,"user_tz":-60,"elapsed":22,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}},"outputId":"79157a21-799b-4ab6-c171-64232c099587"},"id":"rd_8lrXpPab9","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction value, pre-rounding: 0.985539\n","Positive review detected!\n"]}]},{"cell_type":"code","source":["model_name = 'rnn_04_epoch.pth'\n","\n","checkpoint = {'output_size': net.output_size,\n","              'vocab_size':vocab_size,\n","              'embedding_dim': embedding_dim,\n","              'vocab_to_int_': vocab_to_int,\n","              'hidden_dim': net.hidden_dim,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict()}\n","\n","with open(model_name, 'wb') as f:\n","  torch.save(checkpoint, f)"],"metadata":{"id":"DQUQt41CQTlj","executionInfo":{"status":"ok","timestamp":1651343740542,"user_tz":-60,"elapsed":831,"user":{"displayName":"Nicholas Edoseghe","userId":"00121592726822803836"}}},"id":"DQUQt41CQTlj","execution_count":31,"outputs":[]}],"metadata":{"colab":{"name":"RNN Sentimental Analysis.ipynb","provenance":[]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}